<font size=4 face='楷体'>

## 报错汇总

> import flash_attn_2_cuda as flash_attn_cuda fails

- 解决

  ```bash
  # Uninstall flash-attention once
  pip uninstall flash-attn

  # Install flash-attention from source
  git clone https://github.com/Dao-AILab/flash-attention
  cd flash-attention
  python setup.py install
  ```

  或者需要匹配[对应版本](https://github.com/Dao-AILab/flash-attention/releases/tag/v2.5.9.post1)

  > torch 2.3.0 + flash-attn 2.5.8 works

- 参考
  [issues/1588](https://github.com/OpenAccess-AI-Collective/axolotl/issues/1588)
  [flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: \_ZN3c104cuda9SetDeviceEi](https://github.com/Dao-AILab/flash-attention/issues/931)

> AttributeError: 'str' object has no attribute 'model_dump'

- 解决
  by adding "/v1" at the end of openai_api_base parameter.

  ```py
  model = ChatOpenAI(base_url="https://xxxx/v1")
  ```

- [参考]
  [issues/15888](https://github.com/langchain-ai/langchain/issues/15888)

### Reference

**2024.06.26**
