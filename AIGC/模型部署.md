<font size=4 face='楷体'>

## 模型部署

### 模型下载

```python
# model_download.py
import os
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
# model_dir = snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='./models', revision='master')
model_dir = snapshot_download('qwen/Qwen2-72B-Instruct', cache_dir='./models', revision='master')
```

### 模型性能测试

### 模型部署

```bash
python -m vllm.entrypoints.openai.api_server --model ./models/qwen/Qwen2-7B-Instruct  --served-model-name Qwen2-7B-Instruct --max-model-len=15744 --trust-remote-code

python benchmark_throughput.py \
	--model ./models/qwen/Qwen2-7B-Instruct \
	--backend vllm \
	--input-len 64 \
	--output-len 128 \
	--num-prompts 25 \
	--seed 2024 \
    --dtype float16 \
    --max-model-len 512 \
    --trust-remote-code

# https://blog.csdn.net/Hello_World1023/article/details/130355998
# 多卡部署使用 CUDA_VISIBLE_DEVICES 指定显卡
CUDA_VISIBLE_DEVICES=0,1,2 

# Maybe solve the problem of the AsyncEngineDeadError
# vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop has errored already.
# https://github.com/vllm-project/vllm/issues/5376
VLLM_ATTENTION_BACKEND=XFORMERS
python -m vllm.entrypoints.openai.api_server --model /root/dataDisk/models/ZhipuAI/glm-4-9b-chat --served-model-name glm-4-9b-chat --max-model-len=15744 --trust-remote-code

python benchmark_throughput.py \
	--model /root/dataDisk/models/ZhipuAI/glm-4-9b-chat \
	--backend vllm \
	--input-len 64 \
	--output-len 128 \
	--num-prompts 25 \
	--seed 2024 \
    --dtype float16 \
    --max-model-len 512 \
    --trust-remote-code

python -m vllm.entrypoints.openai.api_server --model ./models/qwen/Qwen2-72B-Instruct --tensor-parallel-size 4 --served-model-name Qwen2-72B-Instruct --max-model-len=15744 --trust-remote-code --gpu-memory-utilization 0.9

python benchmark_throughput.py \
	--model ./models/qwen/Qwen2-72B-Instruct \
	--backend vllm \
	--input-len 64 \
	--output-len 128 \
	--num-prompts 25 \
	--seed 2024 \
    --dtype float16 \
    --max-model-len 512 \
    --trust-remote-code \
    --tensor-parallel-size 4

```
```bash
ssh -CNg -L 8000:127.0.0.1:8000 root@60.165.238.46 -p 31408
```

### Reference

**2024.06.18**
