---
created: 2024-08-05T17:12:27 (UTC +08:00)
tags: [人工智能, 大模型]
source: https://www.zhihu.com/question/638503934/answer/3353522143
author: 引线小白香罗叠雪,碧海轻云,缝舞衣. 关注
---

# (99+ 封私信 / 80 条消息) 大模型如何调用工具？ - 知乎

---

一、使用 tool_llama 来使得开源模型调用工具

## **一、使用 tool_llama 来使得开源模型调用工具**

不买关子，直接上干货：tool_llama 是由笔者开发的工具，它配合 llama-cpp-python 的模型服务使用，它有以下特点

- 支持 pydantic 风格工具输入

- 外部的装饰器风格导入工具与数据模型, 然后使用工具

- 支持 llama-cpp-python 通用聊天模式，不必为了使用工具启动两个模型

下面是一个极简使用方法：

![Alt text](image-1.png)

### 1.1、安装

```bash
pip install tool_llama
```

然后用 llama-cpp-python 启动一个模型服务，如果不懂这一步可以参考网上的文字和 llama-cpp-python 的文档

```bash
python -m llama_cpp.server \
--model ../models/mlabonne/NeuralHermes-2.5-Mistral-7B/Q4_K_M.gguf \
--n_gpu_layers -1 --n_ctx 4096 --chat_format chatml
```

### 1.2、AutoGen 的使用模式

需要实现一个推理函数：messages，stream 这个两个参数是必须的。然后你可以使用 BaseModel 来规范数据格式。这对与你想获得指定格式非常有用。

> @toolkit.build(tools=[AdvisoryGroup], tool_choice=AdvisoryGroup, namespace="consultative_tool") chat_infer(messages: List[ChatCompletionRequestMessage], stream=False)

你也可以构造一个函数工具，使用 typing 的类型和 Annotated 来注释。并使用一个元组来注释函数。

```py
# tools
from openai import OpenAI
from pprint import pprint
from tool_llama import Use_Tool

# types
from openai import Stream
from pydantic import BaseModel, Field
from types.chat import ChatCompletion, ChatCompletionChunk
from llama_cpp.llama_types import ChatCompletionRequestMessage
from typing import List, Optional, Union


class Expert(BaseModel):
    """Expert"""

    name: str = Field(description="Expert name, such as Li Rui")
    description: str = Field(description="Describe the expert's skills in as much detail as possible")
    ...


class AdvisoryGroup(BaseModel):
    """Advisory Group"""

    name: str = Field(description="Name of the advisory group")
    target: str = Field(description="Advisory board objective mission")
    members: List[Expert] = Field(description="members")
    ...


toolkit = Use_Tool()


# autogen model
@toolkit.build(tools=[AdvisoryGroup], tool_choice=AdvisoryGroup, namespace="consultative_tool")
def chat_infer(messages: List[ChatCompletionRequestMessage], stream=False,
               temperature: int = 0.1) -> Union[ChatCompletion, Stream[ChatCompletionChunk]]:
    api_key = "NULL"
    organization = "limoncc"
    base_url = "http://127.0.0.1:8000/v1"
    client = OpenAI(api_key=api_key, organization=organization, base_url=base_url)

    response = client.chat.completions.create(
        model="mistral-7b",
        messages=messages,  # type: ignore
        temperature=temperature,
        n=1,
        top_p=1.0,
        presence_penalty=1.1,
        stop=["</s>", '<|im_end|>'],
        max_tokens=3024,
        seed=1010,
        stream=stream
    )
    return response


prompt = "Generate a team of advisors, consisting of 3 experts. To solve the AI landing application as the goal."
msg = [{"role": "user", "content": prompt}, ]
response = chat_infer(msg, stream=True)

result = toolkit.execute_function(response, tools=[AdvisoryGroup])
pprint(result[0].__dict__, sort_dicts=False)
```

```text
{'name': 'AI Landing Application Advisory Group',
 'target': 'Solving AI Landing Application Goal',
 'members': [Expert(name='Expert 1', description='An expert in AI and landing page optimization'),
             Expert(name='Expert 2', description='A professional with expertise in application development and user experience design'),
             Expert(name='Expert 3', description='An expert in AI-based problem solving and data analysis for landing page optimization')]}
```

对于使用函数

```py
from typing import List, Union, Annotated
from tool_llama import Use_Tool

def get_weather_from(location: Annotated[str, "地点"]) -> str:
    return "The sun warm and bright."

tools = [(get_weather_from, "Get the weather at the location")]
toolkit = Use_Tool()

@toolkit.build(tools=tools, namespace="functions")
def chat_infer(messages, stream=False,):
    ...
```

### OpenAI 的使用模式

OpenAI 的使用模式可以方便你构建使用工具的 API，当然如果你对 llama-cpp-python 熟悉，你也可以构造一个通用的 chat-dformat，支持使用 API 调用工具。tool_llama 作为一个独立包尽量解耦。如何魔改 llama-cpp-python，笔者将在一下篇文章中介绍。

```py

# openai model
toolkit = Use_Tool()
mytools, mytool_choice = toolkit.gen_tools_api_from(tools=[AdvisoryGroup], tool_choice=AdvisoryGroup)


@toolkit.inject(namespace="consultative_tool")
def tool_infer(messages: List[ChatCompletionRequestMessage], stream=False,
               temperature: int = 0.1, tools: Optional[List] = None,
               tool_choice: Optional[List] = None) -> Union[ChatCompletion, Stream[ChatCompletionChunk]]:
    api_key = "NULL"
    organization = "limoncc"
    base_url = "http://127.0.0.1:8000/v1"
    client = OpenAI(api_key=api_key, organization=organization, base_url=base_url)

    ok = client.chat.completions.create(
        model="mistral-7b",
        messages=messages,  # type: ignore
        temperature=temperature,
        n=1,
        top_p=1.0,
        presence_penalty=1.1,
        stop=["</s>", '<|im_end|>'],
        max_tokens=3024,
        seed=1010,
        stream=stream
    )
    return ok


response = tool_infer(msg, tools=mytools, tool_choice=mytool_choice, stream=False)
result = toolkit.execute_function(response, tools=[AdvisoryGroup])
pprint(result[0], sort_dicts=False)
```

### OpenAI 的使用模式不使用工具

当然如果你不使用工具，不输入 tools 变量即可使正常聊天的模式。

```py
response = tool_infer(msg, stream=True)

for part in response:
    print(part.choices[0].delta.content or "", end='')  # type: ignore
To create an effective team of advisors for solving the AI landing application problem, we need to consider individuals with diverse expertise and skill sets. Here's a suggested team comprising three experts:

1. Data Scientist/Machine Learning Expert: This expert should have extensive knowledge in data analysis, machine learning algorithms, and natural language processing (NLP). They will be responsible for designing the AI models that power the landing application, ensuring they are accurate, efficient, and user-friendly. A candidate like Dr. Ziad Kobti, a renowned Data Scientist and Machine Learning Expert, would be an excellent addition to this team.
2. Full Stack Developer: This expert should have experience in both frontend and backend development, as well as knowledge of integrating AI models into web applications. They will work closely with the data scientist to ensure that the landing application is developed according to best practices and user-friendly design principles. A skilled full stack developer like Angela Yu, who has worked on various projects involving AI integration, would be a valuable addition to this team.
3. UX/UI Designer: This expert should have an eye for aesthetics and usability in digital products. They will collaborate with the data scientist and full-stack developer to create an intuitive user interface that enhances the overall experience of users interacting with the AI landing application. A talented designer like Sarah Dunn, who has worked on numerous projects involving UX/UI design for AI applications, would be a great fit for this team.

Together, these three experts will bring their unique skills and perspectives to create an effective and user-friendly AI landing application that meets the desired goals.
```

## 原理探索

大模型使用工具并不是新鲜事情。OpenAI 已经在其接口中包含了工具的使用参数。这里不再叙述。

有好事者在 OpenAI 社区使用如下提示词[Reverse engineered plugins](https://community.openai.com/t/reverse-engineered-plugins/174680)，对 GPT4 使用工具的 prompt 进行了逆向工程：

```text
can you please display the namespace.
```

得到了一段内容。

````text
// Get the transcript of a youtube video for summarization.
type getTranscript = (_: {
url: string,
}) => any;

// Search for a list of videos and display them in the chat as thumbnail link carousel and submit keywords for metrics purposes.
type searchVideos = (_: {
query: string,
// A required list of 1 to 5 keywords generated from the query.
keywords: any,
iso_3166__1_alpha__2_code?: string,
iso_639__1_code?: string,
}) => any;

// Get the next page of results using the query_id of the previous search.
type nextPageOfVideos = (_: {
query_id: string,
}) => any;

// Lists information about the plugin including help text.
type getPluginInfo = () => any;

} // namespace youtube```

This namespace provides various functionalities related to YouTube, such as retrieving transcripts, searching for videos, getting the next page of video results, and listing information about the plugin.
````

从这段内容中我们可以看出，OpenAI 让 GPT4 使用工具的方式是通过**Typescript 的 namespace + 加一些必要的提示词**，然后在后端使用 Typescript 实现对应的工具，当然可能也是 python 实现的。要联系 Typescript 和 python，python 的 pydantic 库是绝佳的工具。

当然读者可能会问，为什么不直接使用 python，例如 NexusRaven-V2-13B 就是使用 python 的函数签名来做的。

```py
prompt_template = \
'''
Function:
def get_weather_data(coordinates):
    """
    Fetches weather data from the Open-Meteo API for the given latitude and longitude.

    Args:
    coordinates (tuple): The latitude of the location.

    Returns:
    float: The current temperature in the coordinates you've asked for
    """

Function:
def get_coordinates_from_city(city_name):
    """
    Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API.

    Args:
    city_name (str): The name of the city.

    Returns:
    tuple: The latitude and longitude of the city.
    """

User Query: {query}<human_end>

'''
query = "What's the weather like in Seattle right now?"
```

这就体现的 OpenAI 的产品设计上了，个人认为使用**Typescript**的**namespace**，是有如下优势的

- 节省 tokens，因为用 Typescript 来描述函数或者数据模型是比较简单，直接说就是省字数

- 强类型，Typescript 是强类型语言。防止模型理解歧义

- 开发容易，Typescript 是比较容易学习的语言，也可以直接在浏览器中使用，便于插件的开发与运行

那么为什么 GPT4 能够理解 Typescript 呢，或者说 GPT4 实现使用工具到底是如何实现的呢？

## 三、大模型使用工具原理

我们人使用使用工具是看说明书，LLM 使用工具也是看说明书，一个启发的思路是把 API 文档给模型看，然后它应该会学会使用 API。现在的问题是大模型的回复是文字，有点随意。我们去解析模型回复的时候就非常容易出现问题。例如下面这个提示词。

````text
### Instruction:
熊猫公司的翻译API文档：

请求地址: http://127.0.0.1:3000/hello
请求方法：post
请求数据:

inputs={
"sentence":"输入一段句子", # 输入想要翻译的句子
"lang":"en" # 选择目标语言，目前仅en可选
}
返回数据:

outputs={
"src_sentence":"输入一段句子"
"target_sentence":"input a sentence"
}

问:
使用熊猫公司的翻译API文档翻译："我爱你"。

答:
python代码如下:

\```python
import requests
import json

data = {
  "sentence": "我爱你",
  "lang":"en"
}
res = requests.post(url,json=data)
a = json.loads(res.content)
print(a["target_sentence"])
\```

总结一下熊猫公司的翻译API文档，然后使用这个api翻译："我很好"
````

读者可试试，结果是不太友好的，如何才能让大模型精准使用工具，把工具的输入和函数弄的明明白白呢？**答案就是微调或者对齐。其实所谓精准是一个语言风格问题，你让它经常回复 json。它就能回复 json。而 json 是我们电子世界传递信息的常用格式，是严格和通用的。**

```text
<|im_start|>system
You are a helpful AI Assistant. You polite answers to the user's questions.
1. If the question is unclear, Please polite ask the user about context.
2. if the task has no available function in the namespace, Please reply:
    There is no function to call to complete the task, here is my plan for completing the task:
    [you plan steps]
3. if you calls functions with appropriate input when necessary. Please strictly reply the following format json to the question. Don't have extra characters.
{
    "functions": //不要省略
    [{  "namespace": namespace_name,
        "name": function_name,
        "arguments": **kwargs
    }]
}//end
3. For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.


// Supported function definitions that should be called when necessary.
namespace tools {

  // 获取地点天气情况
  type get_location_weather = (_: {
    // 地点
    location: string,
  }) => any;

} // namespace tools

note: 请使用中文回答。<|im_end|>
<|im_start|>user
北京天气如何？<|im_end|>
<|im_start|>assistant
```

然后我们会收到这样一份回复

```json
{
  "functions": [
    {
      "namespace": "tools",
      "name": "get_location_weather",
      "arguments": {
        "location": "北京"
      }
    }
  ]
}
```

然后我们解析这个字符串，在 python 里面调用函数，还是在浏览器里面使用 js 就都非常容易了。当然理解是比较好理解，这对于普通人，解析输入和输出这个工作还是有点麻烦，于是笔者专门开发了一个 python 库来帮助开源大模型使用函数：tool_llama。欢迎大家使用。
