<font size=4 face='楷体'>

## logstash 配置参考

### 全量/增量同步

- 同步配置

```yaml
# config/mysql-es.conf
input {
   jdbc {
    #数据库连接参数
      jdbc_connection_string => "jdbc:mysql://172.16.188.1:3306/bladex?useSSL=false"
      # mysql用户名
      jdbc_user => "root"
      # mysql密码
      jdbc_password => "123456"
      # mysql驱动器jar包
      jdbc_driver_library => "/var/local/logstash/lib/mysql-connector-java-8.0.22.jar"
      # 驱动类名
      jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
      # 开启分页
      jdbc_paging_enabled => "true"
      # 最大页码
      jdbc_page_size => "50000"
      # 用于同步的查询sql
      statement_filepath => "/var/local/logstash/config/mysql/user.sql"
      # 直接书写sql语句
      # statement => "select * from user"
      # 数据标签，用于标记不同的索引数据
      type => "user"
      # 加上jdbc时区, 要不然logstash的时间会不准确
      jdbc_default_timezone => "Asia/Shanghai"
      # 设置列名区分大小写， 默认全小写
      lowercase_column_names => "false"
    }
}
output {
     if[type] == "user"{
       elasticsearch {
        # elasticsearch url
           hosts => ["172.16.188.7:9200"]
           # 下面两个参数可以开启更新模式
           #action => "update"
           #doc_as_upsert => true
           # 索引名
           index => "user"
           # 文档id 设置成数据库的id
           document_id => "%{id}"
           # 设置文档类型为doc，否则会报错keyword无法匹配到text
           document_type => "_doc"
       }
    }
    stdout {
        # 以json格式输出到控制台，方便调试
        codec => json_lines
    }
}
```

- 增量更新
  则需要在 input/jdbc 下添加如下配置

```yaml
#如果要使用其它字段追踪，而不是用时间开启这个配置
      use_column_value => true
      #设置要追踪的字段
      tracking_column_type => "timestamp"
      tracking_column => "create_time"
      # 是否记录sql_last_value
      record_last_run => true
      #上一个sql_last_value值的存放文件路径, 必须要在文件中指定字段的初始值
      last_run_metadata_path => "/var/local/logstash/config/mysql/user.metadata"
      # cron表达式, 全是*表示每秒都判断是否有更新
      schedule => "* * * * *"
```

```sql
-- config/mysql/user.sql
SELECT
        t.id,
        t.code,
        t.email,
        t.real_name AS realName,
        t.role_id AS roleId,
        t.post_id AS postId,
        t.dept_id AS deptId
FROM
        blade_user t
WHERE  t.create_time > :sql_last_value
```

启动

```bash
./bin/logstash -f config/mysql-es.conf
```

### 分页

```yaml
input {
  jdbc {
    # 激活分页处理
    jdbc_paging_enabled => true
    # 分页大小，每次查询1000条数据
    jdbc_page_size => 1000
    # sql语句
    statement => "SELECT * FROM my_table"
    # ... 忽略其他配置
  }
}
```

### 大表同步

在实际业务场景中，有些数据表的数据会有几百万，甚至上亿的数据，那么在使用 logstash 同步这些大表数据的时候，结合增量同步和分页处理就可以解决，不过需要注意深度分页的性能问题

```sql
-- 每次查询1000条数据，但是翻页从第500万条数据偏移开始
SELECT * FROM my_table limit 5000000, 1000
```

这条 SQL 会非常慢，可以借助索引覆盖优化性能。

```sql
SELECT * FROM my_table WHERE id in (SELECT id FROM my_table limit 5000000, 1000)
```

因为 id 是主键，在主键索引中已经包含 id 的值，不需要回表扫描磁盘的数据，所以性能比较好，上面的 SQL 首先借助索引覆盖将 id 值查询出来，然后根据 id 查询具体的数据。

### 字段

- `es_table`
  这是 MySQL 数据表的名称，数据会从这里读取出来并同步到 Elasticsearch。
- `id`
  这是该条记录的唯一标识符。请注意 “id” 已被定义为 PRIMARY KEY（主键）和 UNIQUE KEY（唯一键）。这能确保每个 “id” 仅在当前表格中出现一次。其将会转换为 “\_id”，以用于更新 Elasticsearch 中的文档及向 Elasticsearch 中插入文档。
- `client_name`
  此字段表示在每条记录中所存储的用户定义数据。在本篇博文中，为简单起见，我们只有一个包含用户定义数据的字段，但您可以轻松添加更多字段。我们要更改的就是这个字段，从而向大家演示不仅新插入的 MySQL 记录被复制到了 Elasticsearch 中，而且更新的记录也被正确传播到了 Elasticsearch 中。
- `modification_time`
  在 MySQL 中插入或更改任何记录时，都会将这个所定义字段的值设置为编辑时间。有了这个编辑时间，我们便能提取自从上次 Logstash 请求从 MySQL 获取记录后被编辑的任何记录。
- `insertion_time`
  此字段主要用于演示目的，并非正确进行同步需满足的严格必要条件。我们用其来跟踪记录最初插入到 MySQL 中的时间。

- `tracking_column`
  此字段会指定 “unix\_ts\_in\_secs” 字段（用于跟踪 Logstash 从 MySQL 读取的最后一个文档，下面会进行描述），其存储在 [.logstash\_jdbc\_last\_run](https://www.elastic.co/guide/en/logstash/7.1/plugins-inputs-jdbc.html#plugins-inputs-jdbc-last_run_metadata_path ".logstash_jdbc_last_run") 中的磁盘上。该值将会用来确定 Logstash 在其轮询循环的下一次迭代中所请求文档的起始值。在 .logstash\_jdbc\_last\_run 中所存储的值可以作为 “:sql\_last\_value” 通过 SELECT 语句进行访问。
- `unix_ts_in_secs`
  这是一个由上述 SELECT 语句生成的字段，包含可作为标准 [Unix 时间戳](https://en.wikipedia.org/wiki/Unix_time "Unix 时间戳")（自 Epoch 起秒数）的 “modification\_time”。我们刚讨论的 “tracking column” 会引用该字段。Unix 时间戳用于跟踪进度，而非作为简单的时间戳；如将其作为简单时间戳，可能会导致错误，因为在 UMT 和本地时区之间正确地来回转换是一个十分复杂的过程。
- `sql_last_value`
  这是一个[内置参数](https://www.elastic.co/guide/en/logstash/7.1/plugins-inputs-jdbc.html#_predefined_parameters "内置参数")，包括 Logstash 轮询循环中当前迭代的起始点，上面 JDBC 输入配置中的 SELECT 语句便会引用这一参数。该字段会设置为 “unix\_ts\_in\_secs”（读取自 .logstash\_jdbc\_last\_run）的最新值。在 Logstash 轮询循环内所执行的 MySQL 查询中，其会用作所返回文档的起点。通过在查询中加入这一变量，能够确保不会将之前传播到 Elasticsearch 的插入或更新内容重新发送到 Elasticsearch。
- `schedule`
  其会使用 cron 语法来指定 Logstash 应当以什么频率对 MySQL 进行轮询以查找变更。这里所指定的 `"*/5 * * * * *"` 会告诉 Logstash 每 5 秒钟联系一次 MySQL。
- `modification_time < NOW()`
  SELECT 中的这一部分是一个较难解释的概念，我们会在下一部分详加解释。
- `filter`
  在这一部分，我们只需简单地将 MySQL 记录中的 “id” 值复制到名为 “\_id” 的元数据字段，因为我们之后输出时会引用这一字段，以确保写入 Elasticsearch 的每个文档都有正确的 “\_id” 值。通过使用元数据字段，可以确保这一临时值不会导致创建新的字段。我们还从文档中删除了 “id”、“@version” 和 “unix\_ts\_in\_secs” 字段，因为我们不希望将这些字段写入到 Elasticsearch 中。
- `output`
  在这一部分，我们指定每个文档都应当写入 Elasticsearch，还需为其分配一个 “\_id”（需从我们在筛选部分所创建的元数据字段提取出来）。还会有一个包含被注释掉代码的 rubydebug 输出，启用此输出后能够帮助您进行故障排查。

### Reference

- [通过 logstash-input-jdbc 实现 mysql8.0 全量/增量同步至 ES7.x](https://blog.51cto.com/u_15952602/6034850)
- [同步 MYSQL 数据到 Elasticsearch](https://www.tizi365.com/archives/721.html)
- [如何使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步](https://blog.csdn.net/UbuntuTouch/article/details/103874185)

**2024.08.02**
