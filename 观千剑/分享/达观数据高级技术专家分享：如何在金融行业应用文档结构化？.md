**导读**

本文整理自达观数据高级技术专家杨慧宇9月3日的直播公开课——《金融文档结构化实践》。直播回放链接：[https://www.bilibili.com/video/BV1KC4y1b7nb](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1KC4y1b7nb)

嘉宾简介：杨慧宇，达观数据高级技术专家，负责 NLP，RPA 等技术在金融行业的实践应用。

随着技术的发展和社会的变革，金融行业在近些年逐渐暴露了各种危机。

![](https://pic3.zhimg.com/v2-f52f7e0df7ab96977c62aeddc1e7b6b6_b.jpg)

**市场竞争**

金融行业虽然在大众印象中是暴利行业，但实际上行业年均复合增长率已经呈现出下降的趋势，并且面临着蚂蚁集团、微信支付、陆金所等互联网金融公司所带来的影响和竞争。

**人力成本**

中国的人口红利在逐渐消失，劳动力成本也在不断上升，尤其像新时代的 90 后员工不愿意天天进行重复性的工作，希望能够在工作中体现出自己独一无二的价值，这一点实际上也是很大的痛点。

**监管趋严**

金融行业领域监管越来越严格。证监会、交易所都在不断地发布新的监管规则，而金融企业内部的合规、风控这些机构就要随之进行更新，这些都离不开信息技术的支持。随着监管的越来越严格，企业内部所造成的文书工作的增加也会带来很多工作上的负担，比如要进行很多信息上的披露，上市公司要发布各种各样事件的曝光，这些都会给大家带来工作上的负担。但是因为文件越来越多，人工处理文本的能力，却无法得到相应的提升，这样就会造成工作上的疏忽，给工作带来潜在的风险。

**技术进步**

现在券商，银行，保险公司等都希望利用人工智能、云计算、大数据、机器人和自动化等技术强化金融行业的数字化能力，并要考量以前的非结构化数据资产能不能发挥应有的价值。  

结合以上趋势，从文档处理的角度出发，本文将探讨相应的解决方案。

![](https://pic1.zhimg.com/v2-dbda53e7a64d27a42e7d2492e79d7a14_b.jpg)

金融行业的文档特征有以下几点：

**文档密集型**

金融行业是文档密集型的行业，不管是券商、银行、保险、基金、私募，这些企业都有大量的文书处理，比如像招股书，债券募集书，这些都是投行业务等行业中会涉及到的。上市企业要提供他的派息公告，股权质押公告，重组公告，审计的会计师事务所要提供相应的审计报告，基金公司会提供基金合同，还有银行会处理各种各样函证，这些都是各种各样的文档。

**文档格式繁杂**

文档格式繁杂，而且往往都是非结构化的文档，比如 word、pdf 与 pdf 的电子版、扫描件，还有图片格式 jpg 这样的文件等等。

**文档处理容错性低**

这是金融行业的特征，很强的监管性导致了该行业对于文档处理的容错性非常低。轻则可能闹出乌龙事件，严重时监管机构可能会对他进行警告，甚至做出罚款，更为严重时，对于券商而言可能会影响客户公司的上市。这些都是文档处理容错性低的体现。

![](https://pic2.zhimg.com/v2-30ecd7cd9173ff843e8680d518ed2099_b.jpg)

上图列举了金融行业曾经犯过的低级错误，债券募集书里提到了高管声明，正常应该说“本募集说明书不存在虚假记载”，例子中出现了重大遗漏，把 “不” 字给漏掉了，导致了意思完全相反；

右上角的图是关于发行超短期融资券的注册文件。但因为人员的疏忽，拼音打字的时候打错了，打成了超短裙。右下这幅图实际上是数字的校验错误，可以看到合计的比例实际上是对不上的。

这些都是由于日常工作中因为疏忽导致的各种各样的风险。文档的机器智能处理，虽然不可能说完全 100% 地去解决各种各样的问题，但是可以有效地去降低人工处理过程中带来的潜在的风险。

讨论了以上问题，接下来的内容将讨论应对的办法 —— 关于金融文档结构化实践，包括三部分：第一部分：关于金融文档解析的算法落地；第二部分：金融文档的信息抽取算法概述；第三部分：达观数据在金融结构化场景中的应用。

## **1\. 关于金融文档解析的算法落地**

![](https://pic2.zhimg.com/v2-3d535463816dc465dd1f24ed2a1e394d_b.jpg)

上图可以看到是刚刚提到的各种各样类型的文档，比如营业执照、发票以及保单这些属于格式相对比较固定的表格形式的文档类型。

另外如上图的银行流水，银行账单，还有其他文件的表格虽然也是表格的形式，但格式很不固定，其文字信息构成是密度很高的结构。还有上图右边的这种股权质押公告，既有表格，又有段落的信息，下面的债券募集书，本身是很长的很规范化的文档，这些都是在实际业务中需要去处理的文档类型。它们的文本结构实际上是五花八门，有表格，有正文，篇章结构等各个方面，基本上你能想象到的，都存在。

![](https://pic4.zhimg.com/v2-c73108389713328ccfaef535397d3293_b.jpg)

以 PDF 为例讲一下的文档解析的具体算法落地。PDF 文件可能既包含的扫描件，也包含 PDF 的可复制的这种电子文档的格式，被广泛应用于日常生活中。这里列举了 Adobe 公司出的 Tutorial，PDF 文件本身的协议是让机器去读的，上图左边协议人无法看懂，因为跟平时看到的 PDF 文件本身的段落、章节、表格是完全不一样的。比如从上图右侧从 root 节点开始到 Catalog，然后里面有各个 Pages，Pages 然后到 Page，每个 Page 里面可能又包含 Resources 项，Resources 里面有可能像字体这种信息，Contents 就是文本的信息。

但这些信息对于人类理解来说，是没有任何意义的。只是规定了诸如”hello world” 这两个单词应该出现在屏幕上的哪个位置，或者某线条应该出现在哪个位置，而这些元素本身没有任何文档信息，和平时读的 PDF 的结构完全不一样。所以这时候要把 PDF 文件转成作为文档的智能解析，就需要找到并处理隐含的各种各样的文档结构信息。

![](https://pic4.zhimg.com/v2-0c5b40fc36c8ff1beca4012dc43fd1c7_b.jpg)

达观数据是将日常人看到的结构转换总结出来，像目录、表格、图片、页眉、页脚，包括段落标题等等。还有论文里面经常出现的公式，甚至像刚看到印章。上图右边是达观数据总结出来静态类图，以这样的数据结构来表示一篇 PDF 文件和其中各种信息，把原本机器才能理解的协议，转化成了人类能够读懂的协议。

文档解析实际上很好地结合了 CV 和 NLP 算法落地的场景，包括软件工程能力。应用看似简单，无非就是表格、段落、章节，看起来好像特征很明显，但实际上对于算法原理、数据结构、工程能力，甚至业务知识、场景理解，都有很高的要求。

![](https://pic2.zhimg.com/v2-58ba47bd5ae2e34bdb127969cd1aae49_b.jpg)

前文提到，文档智能解析包含有很多类型，以表格解析为例来介绍，来看看是怎样一步一步从 machine-readable 变成 human-readable的内容的。

**上图列举了三个财务报表，在金融行业俗称叫三大表，资产负债表、现金流量表和利润表。**三大表信息相对来说比较固定，它的格式对于人来说差不多，但是对于机器来说有很大的差别。比如最左边的很清晰的一个有线框，它的边框表格非常完整，中间是一个扫描件。里面还有盖章，印章对内容有遮盖，而且扫描的质量也不是很清晰，你可以看到最左边和最右边的线条是缺失的状态。

最右边的这幅图完全没有线条，没有财务知识的人，可能会因为信息密集以及没有线条划分，理解时遇到困难。除了信息理解困难，表格中还有只有专业人士才能理解的符号，比如说括号，在财务领域，括号表示的是负号的概念。那像最左边就用负号去表示，这里面的数字底下的横线，实际上也是有特定的财务含义在里面的。虽然说对于人来说大体看起来相同，但实际上对于机器来说，这些都是需要去考虑并且要去做特定的处理，才能达到很完美的解析效果。这种情况下大致的表格解析的流程是：

  
**1.1**

先定位到这样的表格，要框出的表格的大致范围，尤其是对于无框表格来说，没有线条这些信息，那只能通过诸如目标检测这类的 CV 算法，才能够定位到这样的表格：

**1.2**

做线条识别，刚刚你看到有的表格是有线的，有的表格是缺线的，有的表格甚至一条线都没有。这对于线条识别来讲就提出了很高的要求。那有了表格的位置，有了线条，那这时候就可以构造出大致的单元格，注意，还要考虑到是不是有单元格要进行合并。

对于无线来讲，这时候单元格的合并就比较困难，可能要利用 NLP 获取的信息来判断的单元格上下或者是左右，是否是表达连续的内容。

有了这些信息以后，最终可以生成表格，但是到这为止还是远远不够的。有了这样表格以后，在实际业务中还会面临各种各样的问题。假设要去做表格的定位，我要识别资产负债表，那肯定要判断的表头是什么样子。

具体到扫描件，扫描件首先做 OCR 的识别， 而OCR 识别有可能会有错误，比如千分位符和小数点，可能扫描件扫得不清晰，很有可能模型就识别错了。这时候要根据你的业务知识去做文本的纠错，比如说 1 识别成 L，0 识别成了 O，这都是很有可能出现的问题。

![](https://pic4.zhimg.com/v2-b031da6bac488f42c3ba08e38a2b4b03_b.jpg)

上图给出了很极端的例子，正常情况下用扫描件是不会有这种情况的。上图不仅是通过拍摄得到的，而且的拍摄角度不是很理想，有变形，甚至有了揉捏，揉捏后再铺平还会有褶皱，给图像的质量都带来了很大的影响。

第二张图是达观数据做表格定位模型的结果，可以看到基本上是比较好的定位到了表格的范围。

第三幅图就是表格解析的结果。可以看到尽管有各种各样的角度的变化，线条的揉捏，但是依然还是能够把线条识别得很好，这些都是实际情况中可能会遇到的情况，只不过这个例子确实极端了一点。

![](https://pic2.zhimg.com/v2-34713f40dfbdd0617ea8547e5a99aa01_b.jpg)

CCKS2019 的任务是用 PDF 文件内容做结构化，去尝试构建领域的知识图谱。金融领域有海量的公告文件，去做自动化的结构化抽取，满足投研分析、风控、金融监管、事件抽取等等各类需求。在金融领域，目前没有太多文件信息结构化的工具，怎么样能够通过自动化技术来去从各类公告中抽取信息呢？刚才也提到，任务就是从公司定期报告中去提取里面财务报表内各种各样的信息点，那么输入就是很标准的 PDF 文件，里面有各种各样的资产负债表、利润表、现金流量表。输出的是比如像企业的证券代码，证券的简称，还有的各种各样的财务数据。达观凭借刚刚提到的副文本解析上的积累，尤其是在表格解析的能力上面，提出的模型方案整体测试集达到 0.978 的准确率，准确率基本上可以达到工业上可用的状态。

![](https://pic1.zhimg.com/v2-c214457679b60d5ebfd148994fd7946c_b.jpg)

下面来介绍段落解析。NLP 里面有基础任务，比如说分词，是属于序列标注的一类任务。比如说分词里面，举个例子 “他来自达观数据 “，可以分成三个词：他、来自、达观数据。对于每个字我们定义了四个类型， B、M、E、S 分别代表了词的开始、中间和结束，S 表示字单独成词，“他” 就是 S 标签，“来自” 是 BE 标签，“达观数据” 是 BMME 标签，也就是说把分词任务变成了对于每字符来讲的四分类问题，去解决这样的序列标注问题的时候就有很多成熟的算法可以去使用，比如说最早的隐马尔可夫、CRF，还有双向 LSTM 等等，这些都可以用在分词、命名实体识别、词性标注等等这些任务上，这是简单的 NLP 的基础知识的分享。

那么这和段落解析有什么关系呢？段落解析的任务类似于分词任务，字符对应的是文章里面的文本行，段落就是词，把分词任务和段落解析任务做类比，每一行有像分词任务里字符一样的标签：BMES，分别表示段落的起始、中间、结尾，还有独立成段的概念。

刚刚讲到序列标注里面有很多算法，如 CRF、双向 LSTM，包括最新的 BERT 等，都可以在段落解析上尝试使用。段落解析与分词稍微不同的是：分词是要考虑到每字符的上下文的情况，可能逗号在序列集里面就是 S，结合了上下文的语义信息去做概率的判断，但是在分段的时候，实际上每一行的语义信息太过丰富，这在实践上来讲是不太好用的。但分段也有自己的特征，比如说正常的段落，会有比如说像缩进的特征，结尾可能会有标点特征，独立成行的标题可能会有字符的数字开头这样的特征，这些都可以作为特征输入到模型里去。

这样段落解析的问题大概就有了解决方案。请大家思考一个问题，目前段落解析的训练集是很正规的文档，可能解决起来比较简单。但比如像有些从业人员写作的方法不是很正规，导致其生成的文档类型飘忽不定，缩进跟训练集完全不一样，这时候就会遇到各种各样的 badcase，怎么样去解决？难道是重新标注，再训练，再迭代这样的模型吗？当然也是一种方法，但是这样时间成本也会比较高。有的时候甚至说因为不规范的文档和已有正规文档的特征是冲突的时候该怎么样去解决？值得大家思考一下。

![](https://pic2.zhimg.com/v2-b5b9d2b6fca1c65b146d18d437a9d89d_b.jpg)

以上是整个文档解析算法落地的情况，本文以表格解析和段落解析为例，大致介绍了一下流程。另外，像图片、PDF、word，这些都可以去做文档解析。

从非结构化数据变成半结构化数据，有了这些目录、章节、段落、表格等信息以后，实际上就可以去做其他事情，如从业人员在写作的时候，经常会遇到目录和章节不一致的情况，比如：章节改了，目录没改。目录改了，章节没改。而有了这些不一致的信息，就可以去做审核。

再比如：章节信息有 1、2、3、4，有的时候大家是去做简单的复制，复制的时候忘了去修改，同样也可以做审核。但是实际上有这些半结构化数据，还是远远不够的。这时就要在半结构化数据的基础之上，把转变为完全或者说业务上结构化的数据。

![](https://pic3.zhimg.com/v2-1f129c645c09e4a007aa34cc8a95ba62_b.jpg)

比如说金融文档里或者财务报表里，可以去提取像应付账款、实收资本、流动资产这些内容。在提取这些内容以后，首先可以把这些数据存到数据库里面，做结构化存储。第二有了这些内容，可以做比如说像公式上的校验，根据已知的财务公式，可以去计算数字是否有问题。有了这些结构化数据，就有了更多的可能。

## **2\. 金融文档的信息抽取算法概述**

有两种类型的数据，首先是有正文的数据，抽取里面的数字、人名等等信息。另外还有表格里面的数据，表格里面的数据是天然的半结构化结果，要从里面把财务数据的指标数据等提取出来。

这两种文档类型的信息，实际上数据处理方法是不太一样的。简单的说明一下，正文的信息抽取，常见的分词 NER 抽取都是基于一维的文本信息。那常见的方法有刚才介绍过有 CRF、双向 ASTM，bert 系列，各种 bert 的模改的这种模型算法，还有预处理要正则表达式等等，都可以去做这样的信息抽取。但是在实际情况中，只有这些算法是完全不够的，需要加上各种各样的业务知识，还有各种各样的工程落地。

比如说这篇文档有 200 多页，或者像招股书一样 500 多页，bert 本身模型就够大了，那 500、600 页的文档再去做这样的抽取会很慢。这时候是不是可以去利用业务上的相关知识等来做更加工程落地的应用？

![](https://pic2.zhimg.com/v2-48d299d194688a0243d1e4c348c44b65_b.jpg)

比如说前文提到的财务数据，可能就是在某些章节里面才会出现，所以可以去利用的章节信息去做抽取。当然章节信息本身就是业务知识，有财务数据，前面有业务的关键词，也可以去当做这种特征来去使用。

抽取是非常依赖标注的，那怎么样去确保标注质量？这都是在落地中需要考虑的问题。这么多模型，不能靠 bert 解决所有问题，这是不现实的。无论从性能上来讲，还是其他方面都不太现实，这时候需要在不同场景下要去使用不一样的模型。那不同的模型怎么样去做融合，得到最终的输出？实际上这是很 trick 的问题。

在实际使用中，不可避免会有抽取的错误，任何 AI 模型都达不到 100% 的效果，怎么通过业务上的使用，能够让数据越来越准确，让抽取效果越来越准确，形成这样数据闭环呢？这也是在算法落地中必须要考虑的问题。

![](https://pic2.zhimg.com/v2-abf1c4806374d32e2e853235d5878ab1_b.jpg)

文本抽取的算法大家都比较熟悉，许多论文、博客各方面都介绍的非常多。这里给大家以海外发票信息为例，介绍文档抽取算法。海外发票信息这样文本实际上是有很多种不同的表现形式，字段有的出现在左上角、右上角，有的出现在表格里，格式也是千差万别，各个国家的发票可能都不太一样。

如何结合位置特征，结合表格的表头信息，或者本身的语义信息，能够把这些信息综合起来，提高抽取的准确率呢？可以预见，单纯用序列标注去解决这样的问题实际上是不会有太好的效果。

![](https://pic2.zhimg.com/v2-ebb6fd87f435d3b6015ccba2a43877fd_b.jpg)

这时候就需要把整体的页面的布局、文本信息（比如说表格里可能有字体是加粗的，字号可能也会大，存在斜体等）、context 上下文、抽取的单元格表头的样子等各种各样的因素考虑进去。

可以做三种归类：OCR 的文本信息、字体信息和文字本身，文字本身通常指是不是有语义上信息，比如金额就肯定是数字，字段不会出现像汉字或者是英文字符等等。这些细节都是在抽取时要考虑的信息，实际上就是综合各类信息的多模态的抽取。

![](https://pic3.zhimg.com/v2-1d96d1ba04dec54713c18896da9812ea_b.jpg)

![](https://pic3.zhimg.com/v2-7238d3e09aee74c1274d072c57cb6a9e_b.jpg)

今年微软亚研院提出来的 layout language model 的算法，如上图所示。可以看出，实际上跟本文思路非常相似，比如说抽取表单的时候，利用的文本信息，去做 image 的文本信息（text embedding），加了 positionembedding层，就是把每个 OCR 识别出来的字符的框，X0，Y0，即左上角和框的坐标，分别去做了 4 层的 positionembedding，又在基础之上叠加了 Imageembedding，就是图像特征，比如刚才提到的这种字体字号等等这些信息，把这些各种各样特征考虑进来，去做了多模态的语言模型。

对于这种像表格表单，尤其像海外发票这样很多变的场景、文档类型来讲，那么能不能够尝试去提高抽取的效果？layout language model 在训练的时候，跟本文 Bert 也很类似，实际上也是 mask 了字符做抽取，训练的时候是用了 10% 的训练的时间做随机的替换，10% 的时间不改变，还有剩下的 80% 是替换为 mask，跟本文的 Bert 训练的思路是基本上差不多。但同时把各种各样的特征都考虑进去，来提高表格类型这种数据的抽取效果。

![](https://pic2.zhimg.com/v2-b7e8353345e877ab296774b180c73ce1_b.jpg)

前文提到除了常见的算法，还要考虑在实际落地场景中会遇到的各种各样的问题。在表格抽取的时候，主流算法诸如双向 LSTM、CRF 等可能效果没有那么好了。那么就可以去尝试用像微软这种开源的 layout language model 去做尝试。换句话说，要去做信息抽取的任务，你还要去理解算法，要去熟悉的业务知识，能够理解什么样的场景下用什么算法最适合。举一个工程上的例子，比如在 600 页的招股书上单单用 bert 去抽取，肯定性能上是非常非常差的。在实际落地中如上图这五大因素都是要去考虑到的。

## **3\. 达观数据在金融结构化数据领域的应用场景**

最后一部分内容来介绍达观数据在金融结构化数据领域的应用场景。

**企业贷款信息的录入**

企业在去银行贷款的时候，需要提交各种各样的证照。营业执照，开户的许可证，身份证等等各种信息，这些信息往往都是扫描件。这时无论是去做自动化的抽取也好，还是做审核实际上都是非常耗费人力。这种情境下就可以利用本文提到的文档解析和信息抽取的技术，把需要的信息录入信贷系统里。

![](https://pic2.zhimg.com/v2-154dff15d578c465581678c484e6168d_b.jpg)

**企业贷款资料审核**

比如说银行客户经理在给企业客户办贷款时，像银行的普惠金融部，给小微企业做贷款时要评估、出具如上图这样的尽调报告。都需要把信息要去填入到银行内部通常叫信贷管理系统里，来做这样的风控的建模，评估的企业的风险等级、出具净值调查的报告。

这些信息如果靠人力录入效率是非常低的。完全可以完全依赖于信息抽取结构化的能力，做到自动化填列。有了信息的填列以后，还需要去做审核工作。比如审核内部的文档、合同是否合规的？这同样需要先去做要素的抽取，判断条款是否是缺失的。

企业去提交贷款信息的时候，要提交财报，如何判断财报是否是做过手脚？勾稽关系是不是平衡的？资料和原始的文件的信息是不是一致？这些都要去做审核。如果去做人工录入，肉眼审核的效率也是实际上是非常低下的。此种情境下也可以考虑利用机器的抽取的结果来去做审核工作。

![](https://pic4.zhimg.com/v2-d62441fe465287324a9531636f748b7f_b.jpg)

**国际业务资料录入与审核**

像上文提到的海外发票，不像国内的发票，版式相对固定。每个国家甚至每个地区都不太一样，信息分布都不太一样，如果人工录入，需要操作人能够筛别出不同国家的发票，会比正常的信息填入更加的耗时耗力。

![](https://pic4.zhimg.com/v2-d361ca5b4a950ed0f63aeb80017a0503_b.jpg)

**债券募集书，招股说明书的审核**

如上截图也展示出来，把里面的正文里的金额，还有表格里面的金额去做抽取，抽取后可以依照本文的公式、计算表格的占比是否是一致等内容作验证。全文某一个信息提到了好几次，每信息提到的数字是不是应该是要一致的，做上下文的一致性审核。有了结构化的数据以后，这些都成为了可能。

![](https://pic3.zhimg.com/v2-db4b32b67498a9e8a5db33cc6f23c4fa_b.jpg)

以上的应用场景已经非常丰富，最后再讲一点应用，面对企业各种各样的结构化数据，还可以构建知识图谱。

比如说股权关系的图谱、行业的上下游的关系图谱、资金往来甚至人员关系的图谱。达观基于结构化数据的技术，也有这样的类似的产品叫渊海知识图谱，可以去构建金融企业所需要的图谱。有了这些图谱以后，好处首先在于搜索结果能得到比较明确的答案，基于这种数据，建立知识库这样的 QA 系统。

此外，基于企业的资金关系图谱，或者说股权关系图谱，能够评估到企业上下游的是否可能有洗钱或者欺诈的行为，通过这种股权穿透，能够洞察到到企业的风险。这些强大的功能，都是在有了结构化数据的基础之上，才具备了实现的可能，并为之后的更多应用展开打下基石。